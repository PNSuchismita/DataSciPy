{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](img/logo.png)\n",
    "\n",
    "# Recurrent Neural Networks\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will understand:\n",
    "- what recurrent neural network and how they work, and\n",
    "- how memory and state can be implemented in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing this RNN we will follow [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/)'s [blogpost about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness) ([original code gist](https://gist.github.com/karpathy/d4dee566867f8291f086) with BSD License).\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "text = open(filename, 'rt').read()\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations\n",
    "\n",
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `idx_to_char` a dictionary from index to char\n",
    "- `char_to_idx` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array integers representing the chars via their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "idx_to_char = dict(enumerate(chars)) # { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_idx = dict(zip(idx_to_char.values(), idx_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "data = np.array([char_to_idx[c] for c in text], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some of the functions we already know out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.squeeze()\n",
    "    expx = np.exp(x - x.sum())\n",
    "    return expx / expx.sum()\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    return sum([-np.log(p[t]) for p, t in zip(predictions, targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(prev, curr, β):\n",
    "    return [\n",
    "        β * p + (1 - β) * c\n",
    "        for p, c\n",
    "        in zip(prev, curr)\n",
    "    ]\n",
    "    \n",
    "class AdamOptimizer:\n",
    "    def __init__(self, α=0.001, β1=0.9, β2=0.999, ϵ=1e-8):\n",
    "        self.α = α\n",
    "        self.β1 = β1\n",
    "        self.β2 = β2\n",
    "        self.ϵ = ϵ\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def send(self, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [0] * len(gradients)\n",
    "        if self.v is None:\n",
    "            self.v = [0] * len(gradients)\n",
    "\n",
    "        self.t += 1\n",
    "        αt = self.α * np.sqrt(1 - self.β2**self.t) / (1 - self.β1**self.t)\n",
    "        self.m = average(self.m, gradients, self.β1)        \n",
    "        self.v = average(self.v, (g*g for g in gradients), self.β2)\n",
    "\n",
    "        updates = [-αt * mi / (np.sqrt(vi) + self.ϵ) for mi, vi in zip(self.m, self.v)]\n",
    "        for upd in updates:\n",
    "            assert np.isfinite(upd).all()\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model\n",
    "\n",
    "- $x(t)$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h(t)$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $\\widehat y(t)$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "\n",
    "The model is then written as:\n",
    "\n",
    "$$\n",
    "h(t) = \\tanh{\\big(x(t) \\cdot W_x^h + h(t-1) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(t) = softmax\\big(h(t) \\cdot W_h^y\\big)\n",
    "$$\n",
    "\n",
    "and we set $h(0) = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x_t, h_t_1=None):\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    if h_t_1 is None:\n",
    "        h_t_1 = np.zeros(h_size)    \n",
    "    if h_t_1.ndim == 1:\n",
    "        h_t_1 = h_t_1.reshape(-1, 1)\n",
    "    if x_t.ndim == 1:\n",
    "        x_t = x_t.reshape(-1, 1)\n",
    "\n",
    "    # update hidden layer\n",
    "    h_t = np.tanh(Wxh @ x_t + Whh @ h_t_1 + bh)\n",
    "    # fully connected layer\n",
    "    z_t = Why @ h_t + by\n",
    "    z_t = z_t.squeeze()\n",
    "    h_t = h_t.squeeze()\n",
    "    # softmax readout layer\n",
    "    yhat_t = softmax(z_t)\n",
    "    return h_t, z_t, yhat_t\n",
    "\n",
    "def feed_forward(params, x, h0=None):\n",
    "    if h0 is None:\n",
    "        h0 = np.zeros(h_size)\n",
    "    h = {-1: h0}\n",
    "    \n",
    "    shape = (len(x), vocab_size)\n",
    "    x_original = x.copy()\n",
    "    x, z, yhat = np.zeros(shape), np.empty(shape), np.empty(shape)\n",
    "    \n",
    "    for t, char_idx in enumerate(x_original):\n",
    "        x[t, char_idx] = 1.0 # one-hot encoding input into xs  \n",
    "        h[t], z[t, :], yhat[t, :] = step(params, x[t, :], h[t-1])\n",
    "\n",
    "    return x, h, z, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation and \"unrolling\" the network\n",
    "\n",
    "Back propagation works, as before, using the chain rule. \n",
    "It is similar to the [FFN example](FFN.ipynb), except that the $h$ layer adds a bit of complexity, but not much.\n",
    "\n",
    "The details of the gradient calculation can be found in Stanford's [\"Convolutional Neural Networks for Visual Recognition\" course](http://cs231n.github.io/neural-networks-case-study/#grad).\n",
    "\n",
    "What's important to discuss is that instead of back propagating a single step of the network $t$, we back propagate over a sequence of steps, that is over $x=(x_1, \\ldots, x_k)$ for some arbitrary $k$.\n",
    "\n",
    "![rolled RNN](img/rolled_rnn.png)\n",
    "\n",
    "How? By \"unrolling\" the network.\n",
    "\n",
    "![Unrolled RNN](img/unrolled_rnn.png)\n",
    "\n",
    "For example, for $k=3$, the input is $x=[x(1), x(2), x(3)]$, and we can write\n",
    "\n",
    "$$\n",
    "h(1) = \\tanh{\\big(x(1) \\cdot W_x^h + h(0) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(1) = softmax\\big(h(1) \\cdot W_h^y\\big) \\\\\n",
    "h(2) = \\tanh{\\big(x(2) \\cdot W_x^h + h(1) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(2) = softmax\\big(h(2) \\cdot W_h^y\\big) \\\\\n",
    "h(3) = \\tanh{\\big(x(3) \\cdot W_x^h + h(2) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(3) = softmax\\big(h(3) \\cdot W_h^y\\big)\n",
    "$$\n",
    "\n",
    "The cross entropy is computed by summing over all $\\widehat y(t)$ together, and then the gradient is computed for this cross entropy with respect to the various $W$ and $b$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(params, x, y, h0=None):\n",
    "    \"\"\"Calculates loss and gradiens of loss wrt paramters\n",
    "    \n",
    "    See http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of arrays\n",
    "        model parameters\n",
    "    x, y : list of integers\n",
    "        indices of characters for the input and target of the network\n",
    "    h0 : np.ndarray\n",
    "        initial hidden state of shape Hx1\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        value of loss function\n",
    "    dWxh, dWhh, dWhy, dbh, dby \n",
    "        gradients of the loss function wrt to model parameters\n",
    "    h0 : np.ndarray\n",
    "        initial hidden state\n",
    "    \"\"\"\n",
    "    n_inputs = len(x)\n",
    "    # forward pass: compute predictions and loss going forwards\n",
    "    x, h, z, yhat = feed_forward(params, x, h0=h0)\n",
    "    loss = cross_entropy(yhat, y)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    \n",
    "    # back propagate through the unrolled network\n",
    "    for t in reversed(range(len(y))):\n",
    "        # backprop into y\n",
    "        x_t, h_t, yhat_t, y_t = x[t], h[t], yhat[t], y[t] # can't zip because hs is not ordered\n",
    "        dyhat = yhat_t.copy()\n",
    "        dyhat[y_t] -= 1 # Yhat - Y\n",
    "        dWhy += np.outer(dyhat, h_t)  # outer product, same as dy.reshape(-1, 1) @ h.reshape(1, -1)\n",
    "        dby += dyhat.reshape(-1, 1) # dby is a column vector\n",
    "        # backprop into h_t\n",
    "        dh = Why.T @ dyhat + dh_next\n",
    "        # backprop through tanh\n",
    "        dh = (1 - h_t * h_t) * dh # tanh'(x) = 1-x^2\n",
    "        dbh += dh.reshape(-1, 1) # dbh is a column vector\n",
    "        dWxh += np.outer(dh, x_t)\n",
    "        dWhh += np.outer(dh, h[t-1]) # try to use h[t] instead of h[t-1] and see effect in grad_check\n",
    "        dh_next = Whh.T @ dh\n",
    "\n",
    "    gradients = dWxh, dWhh, dWhy, dbh, dby\n",
    "    for grad in gradients:\n",
    "        # clip to mitigate exploding gradients\n",
    "        np.clip(grad, -5, 5, out=grad) # out=grad makes this run in-place\n",
    "    return loss, gradients, h[n_inputs-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer is similar to previous trainers, it goes through the data in `seq_length` batches and then calculates gradients and updates parameters.\n",
    "`seq_length` is not just the batch size for the stochastic gradient descent but also the number of steps to \"unroll\" the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.optimizer = AdamOptimizer()\n",
    "        self.step, self.pos, self.h = 0, 0, None\n",
    "        self.seq_length = seq_length\n",
    "        self.data = data\n",
    "\n",
    "    def train(self, params):\n",
    "        self.step += 1\n",
    "        if self.pos + self.seq_length + 1 >= len(self.data):\n",
    "            # reset data position and hidden state\n",
    "            self.pos, self.h = 0, None\n",
    "        x = self.data[self.pos : self.pos + self.seq_length]\n",
    "        y = self.data[self.pos + 1 : self.pos + self.seq_length + 1]\n",
    "        \n",
    "        loss, gradients, self.h = back_propagation(params, x, y, self.h)\n",
    "        Δs = self.optimizer.send(gradients)\n",
    "        for par, Δ in zip(params, Δs):\n",
    "            par += Δ\n",
    "        self.pos += self.seq_length\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters $W$s and $b$s), a seed char, number of chars, and an state for the hidden layer, produces a sample of text from the network.\n",
    "\n",
    "It does so by using the seed as $x(1)$ and drawing $x(t)$ for $t>1$ from the distribution given by $\\widehat y(t)$.\n",
    "\n",
    "![](img/sampling_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, seed_idx, num_samples, h0=None):\n",
    "    x = np.zeros((num_samples + 1, vocab_size), dtype=float)\n",
    "    x[0, seed_idx] = 1\n",
    "    idx = np.empty(num_samples, dtype=int)\n",
    "    h_t = h0\n",
    "    for t in range(num_samples):\n",
    "        h_t, _, yhat_t = step(params, x[t, :], h_t)\n",
    "        # draw from output distribution\n",
    "        idx[t] = np.random.choice(range(vocab_size), p=yhat_t.ravel())        \n",
    "        x[t + 1, idx[t]] = 1\n",
    "    chars = (idx_to_char[i] for i in idx)\n",
    "    return str.join('', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We can now initialize the parameters and meta-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "max_train_step = 100000\n",
    "\n",
    "# initialize model parameters\n",
    "Wxh = np.random.randn(h_size, vocab_size) * 0.01 \n",
    "Whh = np.random.randn(h_size, h_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, h_size) * 0.01 \n",
    "bh = np.zeros((h_size, 1)) # hidden layer bias\n",
    "by = np.zeros((vocab_size, 1)) # readout layer bias\n",
    "params = Wxh, Whh, Why, bh, by\n",
    "\n",
    "trainer = Trainer(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alsnithe een turf\n",
      "Five beed,\n",
      "\n",
      "RAAHY:\n",
      "OnI yee aceye iron me tith-Whe,\n",
      "Tour celloud magiMed rouete titheg I Ar thein bke toind-brom bu hor oun oun mitrhees that hot cnry magn ist es wpasdyn lud heot.\n",
      "AA\n",
      "\n",
      "train step 10000, loss: 52\n",
      "--------------------------------------------------------------------------------\n",
      "an any pncaalt coaid.\n",
      "\n",
      "FFOS:\n",
      "Hatut, palltepd.\n",
      "\n",
      "IAS:\n",
      "Mat an mao dir s om  hake stoyde eull shee nealdaf.\n",
      "\n",
      "TaLld:\n",
      "Rofld whin sith therulnint bise?l, argebshet bedcere: be ant than ghny go nonle thenliin\n",
      "\n",
      "train step 20000, loss: 66\n",
      "--------------------------------------------------------------------------------\n",
      " my a seast urcis shall'd cragh, Hat? at Mendin tcterer ware my youn\n",
      "And ace lall hiidk rey of but;\n",
      "Mare,\n",
      "Themeed to I hellis:\n",
      "WIa in my. do fome breppeis tneart.\n",
      "\n",
      "FAGBENIS:\n",
      "Chite mrine wishe to ceith\n",
      "\n",
      "train step 30000, loss: 59\n",
      "--------------------------------------------------------------------------------\n",
      "e hycAStcet she prake afrenofist be in lmagstojsent heshe angro Cama,\n",
      "Sid a doceth ghith,\n",
      "I mathat whave's Bon waat ane toutes, wers of to geath soroach shecowind-terofs cere, hemelangr pmish youned c\n",
      "\n",
      "train step 40000, loss: 54\n",
      "--------------------------------------------------------------------------------\n",
      "are Sar whis yout mesen moon:\n",
      "Ghe wandom eerseel ard cmat't would nans wlecose\n",
      "Drethe at betour apes aw tik art rounter thabll thec to kin wey dingmere aan cavy and hay I dout foale at I brerit heams \n",
      "\n",
      "train step 50000, loss: 59\n",
      "--------------------------------------------------------------------------------\n",
      "e hra grould doond;\n",
      "Thak dear she you and hoy the nor dist, icefred was ath preemy, thut ivby carcogis to the Ifor of sat to theov as bupedo\n",
      "Sardim gowreat me me's onsush wo whe me hetheld o say bus.\n",
      "\n",
      "\n",
      "train step 60000, loss: 40\n",
      "--------------------------------------------------------------------------------\n",
      "e fo lace\n",
      "Setwoughe hay.\n",
      "And, a som, myre we bead us be, thevere bo trorc.\n",
      "\n",
      "OKI Nszeill whes yourser!\n",
      "\n",
      "DASINGING TERK\n",
      "Whak thy dond the traked yrerid hint mith dous, me of lord sure ho to situer cour \n",
      "\n",
      "train step 70000, loss: 45\n",
      "--------------------------------------------------------------------------------\n",
      "al laraseg of you, with erchabag, and fo inouk, lowd sout the bumnts and\n",
      "And and cathises vavee grom wath's cnalve nut p and, yould rive bucg batbom mas nock\n",
      "To'd and not thou he thef int, ad-Br;stor \n",
      "\n",
      "train step 80000, loss: 60\n",
      "--------------------------------------------------------------------------------\n",
      "teran: ded crathint womet bakpclout to mnich ath a mangarn fowthe brer a oule uedest beed\n",
      "Yous hovestn's me wing cour ath,\n",
      "And han,\n",
      "I'll\n",
      "And sing in is denderent goo not wies a\n",
      "twent and and seak serr\n",
      "\n",
      "train step 90000, loss: 40\n",
      "--------------------------------------------------------------------------------\n",
      "t denothest; I hike pator:\n",
      "\n",
      "SIAEEN:\n",
      "Thou ssfive hall wrere\n",
      "Then brabjerit, sery\n",
      "Do mist:\n",
      "But eves me\n",
      "mis wom he brot afed\n",
      "Phitlt somengy sithtpee.\n",
      "\n",
      "ROPATLATY:\n",
      "I dlave be bedtanght of thus sereel see h\n",
      "\n",
      "train step 100000, loss: 51\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while trainer.step < max_train_step:\n",
    "    loss = trainer.train(params)\n",
    "    if trainer.step % (max_train_step//10) == 0:\n",
    "        sample_text = sample(params, 0, 200)\n",
    "        print(sample_text)\n",
    "        print()\n",
    "        print('train step {:d}, loss: {:.2g}'.format(trainer.step, loss))\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- TensorFlow [RNN tutorial](https://www.tensorflow.org/tutorials/recurrent)\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- [Obama-RNN](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0) by samim.\n",
    "- [Gradient checking and advanced optimization](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) on Stanford's \"Unsupervised Feature Learning and Deep Learning\" tutorial\n",
    "- [Making a Predictive Keyboard using Recurrent Neural Networks](http://curiousily.com/data-science/2017/05/23/tensorflow-for-hackers-part-5.html) by Venelin Valkov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com) and is part of the [_Data Science with Python_](https://github.com/yoavram/DataSciPy) workshop.\n",
    "\n",
    "The notebook was written using [Python](http://python.org/) 3.6.5.\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
